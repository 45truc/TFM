{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254090fb-5626-47df-a6e0-8770c2109781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3522809a-ebd2-488f-80bd-b970ecc1d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.\\Graph-WaveNet')\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28a8f07-5e9f-4ec7-bc3c-3e1682ad1b79",
   "metadata": {},
   "source": [
    "# Model Class documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fd98bf-68da-43f5-9737-4a37711d345d",
   "metadata": {},
   "source": [
    "```python\n",
    "def __init__(self, device, num_nodes, dropout=0.3, supports=None, do_graph_conv=True,\r\n",
    "                 addaptadj=True, aptinit=None, in_dim=2, out_dim=12,\r\n",
    "                 residual_channels=32, dilation_channels=32, cat_feat_gc=False,\r\n",
    "                 skip_channels=256, end_channels=512, kernel_size=2, blocks=4, layers=2,\r\n",
    "                 apt_size=\n",
    "\n",
    "    @classmethod\r\n",
    "    def from_args(cls, args, device, supports, aptinit, **kwargs):\r\n",
    "        defaults = dict(dropout=args.dropout, supports=supports,\r\n",
    "                        do_graph_conv=args.do_graph_conv, addaptadj=args.addaptadj, aptinit=aptinit,\r\n",
    "                        in_dim=args.in_dim, apt_size=args.apt_size, out_dim=args.seq_length,\r\n",
    "                        residual_channels=args.nhid, dilation_channels=args.nhid,\r\n",
    "                        skip_channels=args.nhid * 8, end_channels=args.nhid * 16,\r\n",
    "                        cat_feat_gc=args.cat_feat_gc)\r\n",
    "        defaults.update(**kwargs)\r\n",
    "        model = cls(device, args.num_nodes, **defaults)\r\n",
    "        ret``` model\n",
    "```10):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd807e57-8128-466f-9c86-6c16e0555615",
   "metadata": {},
   "source": [
    "Here is a series of arguments that need explanation or clarification:\n",
    "\n",
    "*  ```device```: torch device, i.e., the output of ```torch.device('cpu')```.\n",
    "* ```num_nodes ```: node number.\n",
    "* ```in_dim ```:\n",
    "* ```out_dim ```:\n",
    "* ```residual_channels```:\n",
    "* ```dilation_channels```:\n",
    "* ```skip_channels```:\n",
    "* ```end_channels```:\n",
    "* ```blocks```:\n",
    "* ```layers```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f22ba5-6d82-40a1-a142-e1c072fc2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "GWNet = gwnet(DEFAULT_DEVICE,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322881df-b45e-424e-a5c9-10d4883b43ed",
   "metadata": {},
   "source": [
    "## Input documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c450f6-c0c6-481c-b7ac-10352639bf1c",
   "metadata": {},
   "source": [
    "```python\n",
    "def forward(self, x):\r\n",
    "        # Input shape is (bs, features, n_nodes, n_timesteps)\r\n",
    "        in_len = x.size(3)\r\n",
    "        if in_len < self.receptive_field:\r\n",
    "            x = nn.functional.pad(x, (self.receptive_field - in_len, 0, 0, 0))\r\n",
    "        if self.cat_feat_gc:\r\n",
    "            f1, f2 = x[:, [0]], x[:, 1:]\r\n",
    "            x1 = self.start_conv(f1)\r\n",
    "            x2 = F.leaky_relu(self.cat_feature_conv(f2))\r\n",
    "            x = x1 + x2\r\n",
    "        else:\r\n",
    "            x = self.start_conv(x)\r\n",
    "        skip = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c392a0c5-1e98-4cc5-baee-bd658a5ac87e",
   "metadata": {},
   "source": [
    "> Input shape is (bs, features, n_nodes, n_timesteps)\n",
    "* ```bs```: has to be batch size, in our case this has to be each sample\n",
    "* ```features```: values?\n",
    "* ```n_nodes```: has to be the value a each node??\n",
    "* ```n_timesteps```: values at each time step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11c7ab-1b34-41d9-8899-40aa857d101b",
   "metadata": {},
   "source": [
    "# Trainer Class documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfcc08f-af51-48c8-a580-00e3c4b1946d",
   "metadata": {},
   "source": [
    "```python\n",
    "class Trainer():\r\n",
    "    def __init__(self, model: GWNet, scaler, lrate, wdecay, clip=3, lr_decay_rate=.97)\n",
    "\n",
    "    def train(self, input, real_val):\r\n",
    "        self.model.train()\r\n",
    "        self.optimizer.zero_grad()\r\n",
    "        input = nn.functional.pad(input,(1,0,0,0))\r\n",
    "        output = self.model(input).transpose(1,3)  # now, output = [batch_size,1,num_nodes, seq_length]\r\n",
    "        predict = self.scaler.inverse_transform(output):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726e6c7-a224-4a2f-9df6-fc567ef65a5f",
   "metadata": {},
   "source": [
    "# Training Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d18811f-2b76-4166-9c8b-220a804ffe9d",
   "metadata": {},
   "source": [
    "```python\n",
    "def main(args, **model_kwargs):\r\n",
    "    device = torch.device(args.device)\r\n",
    "    data = util.load_dataset(args.data, args.batch_size, args.batch_size, args.batch_size, n_obs=args.n_obs, fill_zeroes=args.fill_zeroes)\r\n",
    "    scaler = data['scaler']\r\n",
    "    aptinit, supports = util.make_graph_inputs(args, device)\r\n",
    "\r\n",
    "    model = GWNet.from_args(args, device, supports, aptinit, **model_k\n",
    "\n",
    "\n",
    "    for _ in mb:\r\n",
    "        train_loss, train_mape, train_rmse = [], [], []\r\n",
    "        data['train_loader'].shuffle()\r\n",
    "        for iter, (x, y) in enumerate(data['train_loader'].get_iterator()):\r\n",
    "            trainx = torch.Tensor(x).to(device).transpose(1, 3)\r\n",
    "            trainy = torch.Tensor(y).to(device).transpose(1, 3)\r\n",
    "            yspeed = trainy[:, 0, :, :]\r\n",
    "            if yspeed.max() == 0: continue\r\n",
    "            mae, mape, rmse = engine.train(trainx, yspeed)\r\n",
    "            train_loss.append(mae)\r\n",
    "            train_mape.append(mape)\r\n",
    "            train_rmse.append(rmse)wargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64291eb3-4e95-47ba-9177-1659987b3a4e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce9fe5e4-f4fc-4c68-8e1a-dba866da5e08",
   "metadata": {},
   "source": [
    "## Data Loader documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c94d30-9d2b-46d5-a82b-a61f59f3d1fd",
   "metadata": {},
   "source": [
    "```python\n",
    "class DataLoader(object):\r\n",
    "    def __init__(self, xs, ys, batch_size, pad_with_last_sample=True):\r\n",
    "        \"\"\"\r\n",
    "        :param xs:\r\n",
    "        :param ys:\r\n",
    "        :param batch_size:\r\n",
    "        :param pad_with_last_sample: pad with the last sample to make number of samples divisible to batch_size.\r\n",
    "        \"\"\"\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.current_ind = 0\r\n",
    "        if pad_with_last_sample:\r\n",
    "            num_padding = (batch_size - (len(xs) % batch_size)) % batch_size\r\n",
    "            x_padding = np.repeat(xs[-1:], num_padding, axis=0)\r\n",
    "            y_padding = np.repeat(ys[-1:], num_padding, axis=0)\r\n",
    "            xs = np.concatenate([xs, x_padding], axis=0)\r\n",
    "            ys = np.concatenate([ys, y_padding], axis=0)\r\n",
    "        self.size = len(xs)\r\n",
    "        self.num_batch = int(self.size // self.batch_size)\r\n",
    "        self.xs = xs\r\n",
    " \n",
    "\n",
    "def load_dataset(dataset_dir, batch_size, valid_batch_size=None, test_batch_size=None, n_obs=None, fill_zeroes=True):\r\n",
    "    data = {}\r\n",
    "    for category in ['train', 'val', 'test']:\r\n",
    "        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))\r\n",
    "        data['x_' + category] = cat_data['x']\r\n",
    "        data['y_' + category] = cat_data['y']\r\n",
    "        if n_obs is not None:\r\n",
    "            data['x_' + category] = data['x_' + category][:n_obs]\r\n",
    "            data['y_' + category] = data['y_' + category][:n_obs]\r\n",
    "    scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std(), fill_zeroes=fill_zeroes)\r\n",
    "    # Data format\r\n",
    "    for category in ['train', 'val', 'test']:\r\n",
    "        data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])\r\n",
    "    data['train_loader'] = DataLoader(data['x_train'], data['y_train'], batch_size)\r\n",
    "    data['val_loader'] = DataLoader(data['x_val'], data['y_val'], valid_batch_size)\r\n",
    "    data['test_loader'] = DataLoader(data['x_test'], data['y_test'], test_batch_size)\r\n",
    "    data['scaler'] = scaler\r\n",
    "    return data       self.ys = ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7424ec2-43d0-4a37-b9de-96361204f5d6",
   "metadata": {},
   "source": [
    "# Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63c3ee48-a222-4a18-901c-95271c572d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['df']>\n",
      "['df']\n",
      "['axis0', 'axis1', 'block0_items', 'block0_values']\n",
      "<HDF5 group \"/df\" (4 members)>\n",
      "Data from axis0:\n",
      "[b'773869' b'767541' b'767542' b'717447' b'717446' b'717445' b'773062'\n",
      " b'767620' b'737529' b'717816' b'765604' b'767471' b'716339' b'773906'\n",
      " b'765273' b'716331' b'771667' b'716337' b'769953' b'769402' b'769403'\n",
      " b'769819' b'769405' b'716941' b'717578' b'716960' b'717804' b'767572'\n",
      " b'767573' b'773012' b'773013' b'764424' b'769388' b'716328' b'717819'\n",
      " b'769941' b'760987' b'718204' b'718045' b'769418' b'768066' b'772140'\n",
      " b'773927' b'760024' b'774012' b'774011' b'767609' b'769359' b'760650'\n",
      " b'716956' b'769831' b'761604' b'717495' b'716554' b'773953' b'767470'\n",
      " b'716955' b'764949' b'773954' b'767366' b'769444' b'773939' b'774067'\n",
      " b'769443' b'767750' b'767751' b'767610' b'773880' b'764766' b'717497'\n",
      " b'717490' b'717491' b'717492' b'717493' b'765176' b'717498' b'717499'\n",
      " b'765171' b'718064' b'718066' b'765164' b'769431' b'769430' b'717610'\n",
      " b'767053' b'767621' b'772596' b'772597' b'767350' b'767351' b'716571'\n",
      " b'773023' b'767585' b'773024' b'717483' b'718379' b'717481' b'717480'\n",
      " b'717486' b'764120' b'772151' b'718371' b'717489' b'717488' b'717818'\n",
      " b'718076' b'718072' b'767455' b'767454' b'761599' b'717099' b'773916'\n",
      " b'716968' b'769467' b'717576' b'717573' b'717572' b'717571' b'717570'\n",
      " b'764760' b'718089' b'769847' b'717608' b'767523' b'716942' b'718090'\n",
      " b'769867' b'717472' b'717473' b'759591' b'764781' b'765099' b'762329'\n",
      " b'716953' b'716951' b'767509' b'765182' b'769358' b'772513' b'716958'\n",
      " b'718496' b'769346' b'773904' b'718499' b'764853' b'761003' b'717502'\n",
      " b'759602' b'717504' b'763995' b'717508' b'765265' b'773996' b'773995'\n",
      " b'717469' b'717468' b'764106' b'717465' b'764794' b'717466' b'717461'\n",
      " b'717460' b'717463' b'717462' b'769345' b'716943' b'772669' b'717582'\n",
      " b'717583' b'717580' b'716949' b'717587' b'772178' b'717585' b'716939'\n",
      " b'768469' b'764101' b'767554' b'773975' b'773974' b'717510' b'717513'\n",
      " b'717825' b'767495' b'767494' b'717821' b'717823' b'717458' b'717459'\n",
      " b'769926' b'764858' b'717450' b'717452' b'717453' b'759772' b'717456'\n",
      " b'771673' b'772167' b'769372' b'774204' b'769806' b'717590' b'717592'\n",
      " b'717595' b'772168' b'718141' b'769373']\n",
      "Data from axis1:\n",
      "[1330560000000000000 1330560300000000000 1330560600000000000 ...\n",
      " 1340840700000000000 1340841000000000000 1340841300000000000]\n",
      "Data from block0_items:\n",
      "[b'773869' b'767541' b'767542' b'717447' b'717446' b'717445' b'773062'\n",
      " b'767620' b'737529' b'717816' b'765604' b'767471' b'716339' b'773906'\n",
      " b'765273' b'716331' b'771667' b'716337' b'769953' b'769402' b'769403'\n",
      " b'769819' b'769405' b'716941' b'717578' b'716960' b'717804' b'767572'\n",
      " b'767573' b'773012' b'773013' b'764424' b'769388' b'716328' b'717819'\n",
      " b'769941' b'760987' b'718204' b'718045' b'769418' b'768066' b'772140'\n",
      " b'773927' b'760024' b'774012' b'774011' b'767609' b'769359' b'760650'\n",
      " b'716956' b'769831' b'761604' b'717495' b'716554' b'773953' b'767470'\n",
      " b'716955' b'764949' b'773954' b'767366' b'769444' b'773939' b'774067'\n",
      " b'769443' b'767750' b'767751' b'767610' b'773880' b'764766' b'717497'\n",
      " b'717490' b'717491' b'717492' b'717493' b'765176' b'717498' b'717499'\n",
      " b'765171' b'718064' b'718066' b'765164' b'769431' b'769430' b'717610'\n",
      " b'767053' b'767621' b'772596' b'772597' b'767350' b'767351' b'716571'\n",
      " b'773023' b'767585' b'773024' b'717483' b'718379' b'717481' b'717480'\n",
      " b'717486' b'764120' b'772151' b'718371' b'717489' b'717488' b'717818'\n",
      " b'718076' b'718072' b'767455' b'767454' b'761599' b'717099' b'773916'\n",
      " b'716968' b'769467' b'717576' b'717573' b'717572' b'717571' b'717570'\n",
      " b'764760' b'718089' b'769847' b'717608' b'767523' b'716942' b'718090'\n",
      " b'769867' b'717472' b'717473' b'759591' b'764781' b'765099' b'762329'\n",
      " b'716953' b'716951' b'767509' b'765182' b'769358' b'772513' b'716958'\n",
      " b'718496' b'769346' b'773904' b'718499' b'764853' b'761003' b'717502'\n",
      " b'759602' b'717504' b'763995' b'717508' b'765265' b'773996' b'773995'\n",
      " b'717469' b'717468' b'764106' b'717465' b'764794' b'717466' b'717461'\n",
      " b'717460' b'717463' b'717462' b'769345' b'716943' b'772669' b'717582'\n",
      " b'717583' b'717580' b'716949' b'717587' b'772178' b'717585' b'716939'\n",
      " b'768469' b'764101' b'767554' b'773975' b'773974' b'717510' b'717513'\n",
      " b'717825' b'767495' b'767494' b'717821' b'717823' b'717458' b'717459'\n",
      " b'769926' b'764858' b'717450' b'717452' b'717453' b'759772' b'717456'\n",
      " b'771673' b'772167' b'769372' b'774204' b'769806' b'717590' b'717592'\n",
      " b'717595' b'772168' b'718141' b'769373']\n",
      "Data from block0_values:\n",
      "[[64.375      67.625      67.125      ... 59.25       69.\n",
      "  61.875     ]\n",
      " [62.66666667 68.55555556 65.44444444 ... 55.88888889 68.44444444\n",
      "  62.875     ]\n",
      " [64.         63.75       60.         ... 61.375      69.85714286\n",
      "  62.        ]\n",
      " ...\n",
      " [67.         59.66666667 69.55555556 ... 63.33333333 67.44444444\n",
      "  61.22222222]\n",
      " [66.75       62.25       66.         ... 63.         67.875\n",
      "  63.5       ]\n",
      " [65.11111111 66.88888889 66.77777778 ... 63.55555556 68.66666667\n",
      "  61.77777778]]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    " \n",
    "#Open the H5 file in read mode\n",
    "with h5py.File('./data/metr-la.h5', 'r') as file:\n",
    "    print(\"Keys: %s\" % file.keys())\n",
    "    a_group_key = list(file.keys())[0]\n",
    "    print(list(file.keys()))\n",
    "     \n",
    "    # Getting the data\n",
    "    data = list(file[a_group_key])\n",
    "    print(data)\n",
    "    print(file['df'])\n",
    "\n",
    "    group = file['/df']\n",
    "    \n",
    "    # Iterate through the members of the group\n",
    "    for member in group:\n",
    "        # Read data from each member\n",
    "        data = group[member][()]\n",
    "        print(f\"Data from {member}:\")\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfac8570-ed2d-4167-8f13-9a612f182ad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File /data/metr-la.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/metr-la.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\TFM\\lib\\site-packages\\pandas\\io\\pytables.py:418\u001b[0m, in \u001b[0;36mread_hdf\u001b[1;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m     exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exists:\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_buf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    420\u001b[0m store \u001b[38;5;241m=\u001b[39m HDFStore(path_or_buf, mode\u001b[38;5;241m=\u001b[39mmode, errors\u001b[38;5;241m=\u001b[39merrors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# can't auto open/close if we are using an iterator\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# so delegate to the iterator\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File /data/metr-la.h5 does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_hdf('/data/metr-la.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8395a955-fac1-4cda-a640-2ed9933f2ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
